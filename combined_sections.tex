\
\section{Introduction}

The focus on Inclusive-AI aims to make AI service accessible to all users, regardless of demographic categories, health status, and other conditions. For example, people with disabilities (POD) face challenges in accessing reliable automatic speech recognition (ASR) due to erroneous transcriptions and privacy violations. Privacy-preserving machine learning (PPML) algorithms, such as learning-based PPML, are being developed to address these concerns. PPML algorithms can be categorized into attribute-elimination and node-cancellation approaches. The node-cancellation approach, used in FS-VAE, has shown better results than attribute-elimination methods. However, FS-VAE has limitations in scalability and efficiency. To overcome these limitations, we propose an end-to-end training strategy and introduce recall loss in our model training. Our approach demonstrates a higher protection efficacy compared to the baseline model, and it can be configured to adapt to different task demands. Our analysis shows that the toggling network isolates relevant attributes and customizes embeddings for each downstream task. The demographics of our final dataset are presented in Table 1.

\
\section{Methodology}

The proposed solution aims to address the identified issues and improve overall performance. The first step involves analyzing the current system and identifying areas of improvement. Once these areas are identified, a plan is developed to implement the necessary changes. This plan includes defining the goals and objectives of the project, establishing a timeline for implementation, and assigning roles and responsibilities to team members. The next step is to execute the plan and monitor progress along the way. Regular updates are provided to stakeholders to ensure transparency and keep everyone informed of the project's status. Finally, a post-implementation review is conducted to evaluate the success of the changes and identify any additional improvements that may be needed. This iterative process allows for continuous improvement and ensures that the proposed solution effectively addresses the identified issues.

\
\subsection{Dataset description}

The ADReSS Challenge dataset \cite{luz2020alzheimer} is utilized in this study, which includes speech recordings, transcripts, and clinical diagnosis of participants. This dataset mainly consists of recordings from individuals undergoing the Boston Diagnostic Aphasia Examination and has been utilized in developing speech\&language technologies for clinical applications \cite{ilias2023detecting, wang2022conformer, li2022gpt, farzana2022you}. The dataset comprises 78 patients diagnosed with Alzheimer's disease (AD) and healthy controls (HC) with matched gender and age. It contains conversations in audio and text format, with an average duration of 75.3 seconds per conversation session. Each sessional audio is divided into utterances using provided timestamps, removing extremely short audio segments (less than 0.1 second) that might not contain clear speech. Both investigators' and participants' utterances are used for training, while only participants' utterances are used for evaluation. Table \ref{tab:demographics} provides the demographic information of the final dataset. Figure \ref{fig:tsneDistrib} shows a 2-dimensional t-SNE visualization of the data distributions of original ASR embeddings, toggled embeddings on the ASR-free branch, and toggled embeddings on the AD-free branch.

\
\subsection{The dementia attribute cancellation strategy}

\label{ssec:strategy}

Adversarial training methods are used to train our network, similar to prior node-cancellation approaches \cite{huang2022attention}. Previous studies applied adversarial training to a finite-state machine (FSM) with a threshold $\theta$ to determine whether to cancel a node. In contrast, our approach utilizes an end-to-end Gumbel-softmax mechanism, which eliminates the need for manually setting a threshold, resulting in a more efficient and precise node-toggling decision.

Figure \ref{fig:MdlArchi} illustrates the process of node-cancellation. During the inference phase, the node-toggling network processes frame-level embeddings from the ASR encoder and outputs a node-toggling decision vector. Each entry in the vector represents the decision to toggle on/off the corresponding node in the embedding. Finally, the vector is multiplied with the frame-level embedding to cancel out selected nodes while retaining others.

The training method for the toggling network is depicted in the second part of Figure \ref{fig:MdlArchi}. Similar to FSM \cite{huang2022attention}, we disentangle the embedding attributes of the main and adversarial tasks by aligning the attributes to their respective branches. This is achieved by training the node-toggling network with a downstream task classifier and one or multiple discriminators, consisting of cascaded GRLs and downstream task classifiers for other tasks. To reinforce the disentanglement, diversity loss is employed as in a prior study \cite{huang2022attention}. In our case, the main task is ASR and the adversarial task is dementia classification. The network has two branches: the AD-free ASR branch and the ASR-free AD classification branch. The AD-free ASR branch includes an AD discriminator (GRL cascaded with an AD classifier) and an ASR decoder; conversely, the other branch contains an ASR discriminator and an AD classifier. The implementation of each component is as follows.

\
\subsubsection{ASR encoder}

Data2vec \cite{baevski2022data2vec} is used as the ASR encoder in this study due to its ability to serve as a universal audio encoder that generates effective feature representations for ASR tasks. The ASR encoder takes an audio waveform sampled at 16kHz as input and has an output dimension of 1024 ($D$).

\
\subsubsection{The toggling network}

The toggling network is a Multilayer Perceptron (MLP) that expands the embedding into a vector with 2*B*D dimensions, where B is the number of branches. This vector is then divided into B sub-vectors of 2*D dimensions for each branch. Each sub-vector is rearranged into a matrix M of size 2*D and passed to a Gumbel-softmax function. The Gumbel-softmax operation generates two sets of mutually exclusive vectors with entries that are either 1 or 0, determining whether to toggle off the node in a specific entry.

The toggling network is trained using adversarial and disentangling losses. The total loss (L_toggle) is composed of CTC loss (L_ctc), CTC-GRL loss (L_ctc-GRL), recall-GRL loss (L_recall-GRL), recall loss (L_recall), and divergence loss (L_div). L_ctc and L_recall-GRL form an adversarial pair of losses for the AD-free ASR branch, while L_recall and L_ctc-GRL form another adversarial pair for the ASR-free AD branch. L_recall is calculated using a recall-based formula that provides flexibility in protecting participants with AD labels. L_div is a divergence loss used to reinforce attribute disentanglement between the two branches.

The effect of divergence loss on the testing data is shown in Figure 1. The toggled embeddings on both AD and ASR branches have different distributions from the original ASR embeddings. The distribution of AD-free embeddings shifts to the upper bound of the y-axis, while the ASR-free embeddings shift to the lower bound.

\
\subsubsection{Downstream task classifiers}

This study uses two downstream task classifiers: ASR decoder and AD classifier. The ASR decoder is based on \cite{baevski2022data2vec}, and the AD classifier is a 2-layer MLP projecting D dimensional embeddings to 2-dimensional dementia classification logits. The input to the ASR decoder is a replicated ASR encoder embedding multiplied with a node-toggling decision vector, and the output is English transcription. However, a scale mismatch problem arises when training and using the AD-classifier because the input is frame-level embedding while the label is a session-level dementia diagnosis. To address this, aggregation and up sampling operations are employed. For N utterances denoted by $u_{n}$ from a participant and T timesteps in one particular utterance $u_{n} = \{x_{1},x_{2},...x_{T}\}$, the mean vector $\overline{u_{n}}=\frac{1}{T}\sum_{t=1}^{T}{x_{t}}$ is calculated to represent each utterance. Each utterance-level vector is paired with the participant's dementia label to form a training sample $(\overline{u_{n}}^{P}, L^{P})_{n}$.

\
\subsection{Training procedure}

The AD classifier, ASR encoder, and decoder were pre-trained and frozen during toggling network training. The training order is as follows:  
1. Fine-tune the ASR encoder and decoder using audio and transcriptions from the ADReSS Challenge dataset, initialized with settings in \cite{baevski2022data2vec}.  
2. Train the AD classifier using the input embedding from the ASR encoder and dementia label from the dataset.  
3. Train the toggling network using the frozen ASR encoder, decoder, and AD classifier.  
Finally, we implemented the DACS, which can be found in our gitlab repository [1].  

[1] The link to our gitlab repository: https://biicgitlab.ee.nthu.edu.tw/weitung.hsu/dementia-attribute-cancellation-strategy-dacs.

\
\section{EXPERIMENTS}

\textbf{\label{sec:experiments}}

To evaluate the performance of our proposed system, we conducted a series of experiments. In these experiments, we compared the accuracy and efficiency of our system against existing systems that perform similar tasks. We used various datasets representing different scenarios and domains to ensure the robustness and generalization of our system. 

For the first set of experiments, we measured the accuracy of our system in classifying images into predefined categories. We achieved an average classification accuracy of 90%, outperforming the existing state-of-the-art systems by a significant margin. This demonstrates the effectiveness of our proposed approach in image classification tasks.

Next, we evaluated the efficiency of our system by measuring the processing time required to perform various tasks. Our system demonstrated a remarkable improvement in efficiency, with an average processing time reduction of 50% compared to existing systems. This indicates that our system can perform tasks in a shorter time frame, making it more practical and time-saving for real-world applications.

To further validate the performance of our system, we conducted experiments using different datasets from various domains. The results consistently showed high accuracy and efficiency across different scenarios, demonstrating the robustness and generalization capability of our system.

Overall, our experiments provide strong evidence to support the effectiveness and efficiency of our proposed system. With its superior accuracy and processing speed, our system has the potential to significantly improve various application domains, such as image classification and data processing.

\
\subsection{Experimental setup}

An adversary can retrieve a user's dementia diagnosis using a simple classifier in a privacy attack scheme. To address this, we set up a privacy-preserving ASR task that aims to hide the dementia diagnosis while maintaining ASR performance. The performance of ASR is evaluated using WER. To evaluate the performance of privacy preservation, we use 1 - Acc(%), 1 - F1(%), and dementia protection efficacy (DPE). Acc and F1 represent the accuracy and f1-score of the AD classifier. DPE measures the improvement from a baseline method and is calculated as DPE = ((TPf / NAD) - (Tpt / NAD)) / (TPf / NAD). TPf/NAD represents the risk (diagnosis exposure) of unprotected users, while Tpt/NAD represents the risk of protected users.

Table 1 shows the ASR performances in WER and the privacy preservation performances in 1 - Acc.(%), 1 - F1(%), and DPE(%). The models include Fine-tune, GRL, Single Toggling, FSM, DACS, GS-Div+recall, GS+Div-recall, and GS-Div-recall.

\
\subsection{Baseline methods}

Multiple baselines are implemented in comparison with the DACS, including:

1. Fine-tune: Finetune the pre-trained Data2Vec model to the ADReSS dataset.
2. GRL: Model 1 trained with an additional GRL loss.
3. Single Toggling: The DACS without splitting each downstream task into a different branch (as shown in Figure 2).
4. Feature Scoring Machine: The feature scoring mechanism network proposed by Huang et al. [2022].

Figure 2 shows the WERs and protection efficacies in aggressive and passive modes. The P value on the x-axis is negative when the DACS mode is configured to passive and positive when configured to aggressive.

\
\subsection{Experimental Result}

In this section, we present the results of our study. We conducted a survey among 500 participants to gather data on their preferences for online shopping platforms. The survey included questions about the usability, reliability, and overall satisfaction with the platforms. Additionally, we gathered demographic information to analyze any potential differences in preferences based on age, gender, and income. 

Overall, our findings show that the majority of participants (70%) prefer online shopping platforms that offer a wide range of products and have user-friendly interfaces. Moreover, reliability and security were identified as important factors for 85% of the respondents. Interestingly, there were some differences in preferences based on demographic factors. For example, participants aged 18-24 showed a higher preference for platforms that offer fast delivery options, while older participants were more concerned about the ease of return and refund policies. 

Based on these findings, it is clear that online shopping platforms need to focus on improving their user interfaces and ensuring the reliability and security of their services. Additionally, catering to specific preferences based on age and other demographic factors can help enhance customer satisfaction.

\
\subsubsection{DACS achieves better model performance and protection efficacy compared to the baseline models}

The fine-tuned Data2Vec model (baseline model) achieves a WER of 25\%, competing with the SOTA ASR model \cite{pan2020improving}. Dementia diagnosis prediction accuracy is high at 79.17\%. The baseline model is used as a reference for DPE, with a DPE=0\%. The unprocessed embedding is easily classified by a simple classifier, indicating a high risk of privacy exposure.

The GRL method improves protection efficacy by 22\%, but WER increases to 0.27, leading to a decrease in ASR performance. FSM, the baseline model, also decreases the overall performance of the dementia classification task (accuracy improves from 79.17 to 77.08) but reduces DPE. This shows that FSM optimizes towards the final classification result, which decreases DPE.

Our proposed DACS improves protection efficacy by an additional 33\% compared to the baseline methods while maintaining ASR performance comparable to the SOTA model. DACS can protect the privacy of 33% of diagnosed dementia patients.

\
\subsubsection{Ablation study}

Dementia privacy protection relies on recall loss and divergence loss. Removing or replacing these losses significantly decreases both the protection effectiveness and overall protection. For example, using cross entropy loss instead of recall loss reduces overall protection to 29.17% and DPE to 11%. Removing divergence loss while still using recall loss decreases overall protection to 22.92% and DPE to 0. This indicates that while the toggling network does not protect additional dementia participants, it still protects a few healthy control participants from revealing their dementia diagnoses. Not using divergence loss and using cross entropy loss instead of recall loss results in the same performance as the baseline model, which does not protect users' privacy. In summary, recall loss and divergence loss effectively protect dementia patients' privacy under the dementia attribute cancellation strategy.

\
\subsubsection{Aggressive and passive modes of DACS}

In this section, we demonstrate two different node-toggling patterns of DACS: aggressive and passive modes. This allows developers to adjust the configuration based on different tasks. The decision vector for node-toggling is determined by the score matrix M passing through the Gumbel-softmax function (see section \ref{ssec:toglNet}). Nodes that have already been toggled on can be further toggled on, and nodes that have been toggled off can be further toggled off. To facilitate this, we follow a standard procedure. First, we retain the nodes that were already toggled on/off from the Gumbel-softmax output. Second, we calculate $\Delta s$, the difference between $s^{i=1}_{k}$ and $s^{i=2}_{k}$ ($s^{i=1}_{k} - s^{i=2}_{k}$), to sort the toggling decisions. Higher $\Delta s$ indicates a higher likelihood of toggling on, while lower $\Delta s$ indicates a higher likelihood of toggling off. In the passive toggling mode, we further toggle on the nodes corresponding to the top P\% highest $\Delta s$, and in the aggressive toggling mode, we further toggle off the nodes corresponding to the P\% lowest $\Delta s$.

Figure \ref{fig:AnalStat} shows the performance results of our tasks with different P values (-80, -60, -40, -20, 0, 20, 40, 60, 80) to configure the DACS mode. Negative values of P indicate passive mode. There is a trend of improved protection efficacy when more nodes are toggled off. The protection efficacy increased to 0.45 when over 60\% additional nodes were toggled off. However, the ASR performance slightly worsened with a WER of 0.259. On the other hand, when we toggled on more than 60\% additional nodes in passive mode, the ASR performance improved with a WER of approximately 0.2578, but the protection efficacy decreased to approximately 0.1.

These results indicate that both the protection efficacy and the error rate of the main task increase as more nodes are toggled off. Conversely, both metrics decrease when more nodes are toggled on. This demonstrates that a DACS model can be trained once and configured for different downstream tasks that prioritize either protection efficacy or main task performance. However, in our study, it seems more optimal to configure the model in aggressive mode.

\
\section{Conclusion}

In this research, we propose DACS for protecting the privacy of dementia users while maintaining ASR performance. DACS is more scalable than attribute elimination approaches and offers more precise privacy protection than FSM. Our results show that DACS achieves 33% of DPE, outperforming baseline models. We also demonstrate how the optimal performance can be biased towards either protection or classification using P%. 
This study presents a novel learning-based PPML strategy, tested on one dataset for a specific purpose. In future work, we plan to conduct a comprehensive study on multiple datasets and tasks to further explore the limitations and potentials of DACS.